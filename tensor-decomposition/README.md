# æœºå™¨å­¦ä¹ ä¸­çš„å¼ é‡æ–¹æ³•(Tensor Methods in Machine Learning)

This original blog is from https://www.offconvex.org/2015/12/17/tensor-decompositions/.

æœ¬æ–‡ç¿»è‡ªhttps://www.offconvex.org/2015/12/17/tensor-decompositions/ã€‚

ç¿»è¯‘ï¼š å¼ äºšä¸œ


- [çŸ©é˜µåˆ†è§£ ï¼ˆMatrix Decompositionsï¼‰](#1-matrix-decompositions)
- [åˆ†è§£çš„ä¸ç¡®å®šæ€§ ï¼ˆThe Ambiguityï¼‰](#2-the-ambiguity)
- [å¢åŠ ç¬¬ä¸‰ä¸ªç»´åº¦ï¼ˆAdding the 3rd Dimensionï¼‰](#3-adding-the-3rd-dimension)
- [æŒ–æ˜å¼ é‡ï¼ˆFinding the Tensorï¼‰](#4-finding-the-tensor)
- [å¼ é‡åˆ†è§£çš„å®ç°ï¼ˆImplementing Tensor Decompositionï¼‰](#5-implementing-tensor-decomposition)

![img](img/mindplot.jpeg)

Tensors are high dimensional generalizations of matrices.
In recent years tensor decompositions were used to design learning algorithms for estimating parameters of latent variable models like Hidden Markov Model, Mixture of Gaussians and Latent Dirichlet Allocation (many of these works were considered as examples of â€œspectral learningâ€, read on to find out why). 
In this post I will briefly describe why tensors are useful in these settings.

å¼ é‡æ˜¯çŸ©é˜µçš„é«˜ç»´æ¨å¹¿ã€‚åœ¨è¿‘äº›å¹´ï¼Œå¼ é‡åˆ†è§£è¢«å¹¿æ³›åº”ç”¨äºä¸ºé‚£äº›å…·æœ‰æ½œåœ¨å˜é‡çš„æ¨¡å‹è®¾è®¡å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚éšè—é©¬å°”ç§‘å¤«æ¨¡å‹ï¼ˆHMMï¼‰ã€é«˜æ–¯å’Œéšè—Dirichletåˆ†é…çš„æ··åˆæ–¹æ³•ã€‚é€šå¸¸ï¼Œè¿™äº›æ–¹æ³•éƒ½è¢«è®¤ä¸ºæ˜¯è°±å­¦ä¹ çš„å®ä¾‹ã€‚
åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä¼šç®€å•ä»‹ç»ä¸ºä½•å¼ é‡åœ¨è¿™äº›é¢†åŸŸå¦‚æ­¤ç¥é€šå¹¿å¤§ã€‚

Using Singular Value Decomposition (SVD), we can write a matrix MâˆˆRnÃ—m as the sum of many rank one matrices:

é€šè¿‡å¥‡å¼‚å€¼åˆ†è§£æ–¹æ³•ï¼ˆSVDï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠä¸€ä¸ªçŸ©é˜µ <img src="http://latex.codecogs.com/gif.latex?\textbf{M}%20\in%20R_{n%20\times%20m}" />  è®°ä½œå¤šä¸ªç§©ä¸º1çš„çŸ©é˜µå’Œçš„å½¢å¼ï¼š
<div align=center>
<img src="http://latex.codecogs.com/gif.latex?\textbf{M}=\sum_{i=1}^{r}{\lambda_i\overrightarrow{u_i}\overrightarrow{v_i}^{T}}" />
</div>

When the rank r is small, this gives a concise representation for the matrix M (using (m+n)r parameters instead of mn). Such decompositions are widely applied in machine learning.

å¦‚æœ<img src="http://latex.codecogs.com/gif.latex?r" />æ¯”è¾ƒå°ï¼Œé‚£ä¹ˆå°±å¯ä»¥ç»™å‡ºå¯¹çŸ©é˜µ<img src="http://latex.codecogs.com/gif.latex?\textbf{M}">çš„ç®€æ´è¡¨è¾¾å½¢å¼ï¼ˆä½¿ç”¨äº†<img src="http://latex.codecogs.com/gif.latex?(m+n)\times%20r" />ä¸ªå‚æ•°ï¼Œè€Œä¸æ˜¯<img src="http://latex.codecogs.com/gif.latex?m\times%20n" />ä¸ªï¼‰ã€‚è¿™æ ·çš„åˆ†è§£åœ¨æœºå™¨å­¦ä¹ ä¸­æœ‰ç€å¹¿æ³›åº”ç”¨ã€‚

Tensor decomposition is a generalization of low rank matrix decomposition. Although most tensor problems are NP-hard in the worst case, several natural subcases of tensor decomposition can be solved in polynomial time. Later we will see that these subcases are still very powerful in learning latent variable models.

å¼ é‡åˆ†è§£æ˜¯ä½ç§©çŸ©é˜µåˆ†è§£çš„æ¨å¹¿ã€‚å°½ç®¡å¤§å¤šæ•°å¼ é‡é—®é¢˜åœ¨æœ€åçš„æƒ…å†µä¸‹éƒ½æ˜¯NPéš¾é—®é¢˜ï¼Œä½†æ˜¯å¾ˆå¤šå¼ é‡åˆ†è§£çš„å­é—®é¢˜å¯ä»¥åœ¨å¤šé¡¹å¼çº§åˆ«å¤æ‚åº¦çš„æ—¶é—´å†…è§£å†³ã€‚åé¢æˆ‘ä»¬ä¼šçœ‹åˆ°ï¼Œè¿™äº›å­é—®é¢˜åœ¨å­¦ä¹ æ½œåœ¨å˜é‡æ¨¡å‹ä¸­ä»æ—§ååˆ†å¼ºå¤§ã€‚

## 1. çŸ©é˜µåˆ†è§£ ï¼ˆMatrix Decompositionsï¼‰

Before talking about tensors, let us first see an example of how matrix factorization can be used to learn latent variable models. In 1904, psychologist Charles Spearman tried to understand whether human intelligence is a composite of different types of measureable intelligence. Letâ€™s describe a highly simplified version of his method, where the hypothesis is that there are exactly two kinds of intelligence: quantitative and verbal. Spearmanâ€™s method consisted of making his subjects take several different kinds of tests. Letâ€™s name these tests Classics, Math, Music, etc. The subjects scores can be represented by a matrix M, which has one row per student, and one column per test.

åœ¨è®²å¼ é‡ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸ªä¾‹å­å…³äºå¦‚ä½•å°†çŸ©é˜µåˆ†è§£åº”ç”¨äºå­¦ä¹ éšè—å˜é‡ã€‚
åœ¨1940å¹´ï¼Œå¿ƒç†å­¦å®¶Charles Spearmanå°è¯•ç†è§£äººç±»çš„æ™ºåŠ›æ˜¯å¦å¯ä»¥åˆ†è§£ä¸ºå¤šç§å¯æµ‹é‡çš„æ™ºåŠ›ç§ç±»ã€‚æˆ‘ä»¬æ¥ä»‹ç»ä¸€ä¸ªå¯¹Charles Spearmanæ–¹æ³•çš„ä¸€ä¸ªé«˜åº¦ç®€åŒ–çš„ç‰ˆæœ¬â€”â€”å‡è®¾äººç±»çš„æ™ºåŠ›ç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼Œé‡åŒ–å’Œè¯­è¨€ã€‚
Charles Spearmané€šè¿‡è¿›è¡Œå„ç§ä¸åŒç±»å‹çš„è€ƒè¯•æ¥ç»„æˆä»–çš„ç§‘ç›®ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼šæ–‡å­¦ï¼Œæ•°å­¦ï¼ŒéŸ³ä¹ç­‰ç­‰ã€‚è¿™äº›è¯¾ç¨‹çš„åˆ†æ•°ç”¨çŸ©é˜µMæ¥è¡¨ç¤ºï¼Œå…¶ä¸­ï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå­¦ç”Ÿï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ç§ç§‘ç›®ã€‚

<div align=center>
<img src="img/1_1.png" />
</div>

The simplified version of Spearmanâ€™s hypothesis is that each student has different amounts of quantitative and verbal intelligence, say xquant and xverb respectively. Each test measures a different mix of intelligences, so say it gives a weighting yquant to quantitative and yverb to verbal. Intuitively, a student with higher strength on verbal intelligence should perform better on a test that has a high weight on verbal intelligence. Letâ€™s describe this relationship as a simple bilinear function:

Charles Spearmançš„å‡è®¾çš„ç®€åŒ–ç‰ˆï¼šæ¯ä¸€ä¸ªå­¦ç”Ÿå…·æœ‰ä¸åŒçš„é‡åŒ–æ™ºåŠ›æ°´å¹³å’Œè¯­è¨€æ™ºåŠ›æ°´å¹³ï¼Œå„è‡ªç”¨<img src="http://latex.codecogs.com/gif.latex?x_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?x_{verb}" /> ç¬¦å·ä»£è¡¨ã€‚
åŒæ—¶ï¼Œæ¯ä¸€ç§ç§‘ç›®çš„æµ‹è¯•è¡¡é‡äº†ä¸åŒç¨‹åº¦æ™ºåŠ›æ°´å¹³çš„ç»„åˆï¼Œï¼ˆä¾‹å¦‚æ•°å­¦ç§‘ç›®å½“ä¸­è¡¡é‡é‡åŒ–æ™ºåŠ›æ°´å¹³çš„ç¨‹åº¦è¦æ›´é«˜ä¸€äº›ï¼‰ï¼Œæ‰€ä»¥ç»™é‡åŒ–æ°´å¹³å’Œè¯­è¨€æ°´å¹³ä¸åŒçš„æƒé‡ï¼Œåˆ†åˆ«ç”¨<img src="http://latex.codecogs.com/gif.latex?y_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?y_{verb}" /> ä»£è¡¨ã€‚
ç›´è§‚å¾—è¯´ï¼Œä¸€ä¸ªå­¦ç”Ÿå¦‚æœæ‹¥æœ‰è¾ƒé«˜çš„è¯­è¨€æ°´å¹³ï¼ˆç›¸å¯¹äºé‡åŒ–æ°´å¹³ï¼‰ï¼Œé‚£ä¹ˆä»–/å¥¹åº”è¯¥å¯ä»¥åœ¨è¯­è¨€æ°´å¹³æƒé‡æ¯”è¾ƒé«˜çš„ç§‘ç›®ä¸­è·å¾—æ›´ä¼˜å¼‚çš„æˆç»©ã€‚ï¼ˆä¾‹å¦‚è¯­è¨€æ°´å¹³æ›´é«˜çš„åŒå­¦çš„è¯­æ–‡æˆç»©åº”è¯¥è¦æ¯”æ•°å­¦æˆç»©æ›´é«˜ä¸€äº›ï¼‰ã€‚
é‚£ä¹ˆæˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•çš„äºŒç»´çº¿æ€§å‡½æ•°æ¥æè¿°è¿™ä¸ªç°è±¡ï¼š

<div align=center>
<img src="http://latex.codecogs.com/gif.latex?score=x_{quant}%20\times%20y_{quant}+x_{verb}%20\times%20y_{verb}" />
</div>

Denoting by x verb,x quant the vectors describing the strengths of the students, and letting y verb,y quant be the vectors that describe the weighting of intelligences in the different tests, we can express matrix M as the sum of two rank 1 matrices (in other words, M has rank at most 2):

ç”¨<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{x}_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{x}_{verb}" /> å‘é‡è¡¨ç¤ºå­¦ç”Ÿæ ·æœ¬çš„ä¸¤ç§æ™ºåŠ›æ°´å¹³ï¼Œç”¨<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{y}_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{y}_{verb}" />è¡¨ç¤ºåœ¨ä¸åŒè€ƒè¯•å½“ä¸­ä¸¤ä¸ªæ™ºåŠ›ç±»å‹çš„æƒé‡ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸¤ä¸ªç§©ä¸º1çš„çŸ©é˜µå’Œè¡¨ç¤ºMï¼Œï¼ˆè€ŒMçš„ç§©æœ€å¤§ä¸º2ï¼‰ï¼š

<div align=center>
<img src="http://latex.codecogs.com/gif.latex?\textbf{M}=\overrightarrow{x}_{quant}%20\overrightarrow{y}^T_{quant}+\overrightarrow{x}_{verb}%20\overrightarrow{y}^T_{verb}" />
</div>

Thus verifying that M has rank 2 (or that it is very close to a rank 2 matrix) should let us conclude that there are indeed two kinds of intelligence.

å› æ­¤ï¼Œç¡®è®¤çš„çŸ©é˜µ<img src="http://latex.codecogs.com/gif.latex?\textbf{M}">çš„ç§©ä¸º2ï¼ˆæˆ–è€…è¯´æ˜¯å¾ˆæ¥è¿‘ç§©ä¸º2çš„çŸ©é˜µï¼Œè¿™é‡ŒæŒ‡çš„æ˜¯çŸ©é˜µåœ¨ç§‘ç›®æ–¹å‘ä¸Šçš„ç§©å¾ˆæ¥è¿‘2ï¼‰å¯ä»¥è®©æˆ‘ä»¬æ¨æ–­å‡ºç¡®å®åªæœ‰è¿™ä¸¤ç§æ™ºåŠ›æ°´å¹³ã€‚

<div align=center>
<img src="img/1_2.png" />
</div>

Note that this decomposition is not the Singular Value Decomposition (SVD). SVD requires strong orthogonality constraints (which translates to â€œdifferent intelligences are completely uncorrelatedâ€) that are not plausible in this setting.

éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œè¿™é‡Œçš„åˆ†è§£å¹¶ä¸æ˜¯å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ï¼ŒSVDå…·æœ‰å¾ˆå¼ºçš„æ­£äº¤æ€§çº¦æŸæ¡ä»¶ï¼Œæ¢å¥è¯è¯´å°±æ˜¯ä¸åŒçš„æ™ºåŠ›ç±»å‹æ˜¯å®Œå…¨ä¸ç›¸å…³çš„ï¼Œè€Œåœ¨è¿™ä¸ªé—®é¢˜ä¸­æ­£äº¤çº¦æŸå¹¶ä¸åˆç†ã€‚


## 2. åˆ†è§£çš„ä¸ç¡®å®šæ€§ ï¼ˆThe Ambiguityï¼‰

But ideally one would like to take the above idea further: we would like to assign a definitive quantitative/verbal intelligence score to each student. This seems simple at first sight: just read off the score from the decomposition. For instance, it shows Alice is strongest in quantitative intelligence.

å¾ˆè‡ªç„¶åœ°ï¼Œå¤§å®¶ä¼šå°†ä¸Šä¸€èŠ‚è®²åˆ°çš„æ€è·¯è¿›ä¸€æ­¥æ‹“å±•ï¼Œæˆ‘ä»¬å¸Œæœ›ç»™æ¯ä¸€ä¸ªå­¦ç”Ÿèµ‹äºˆç¡®å®šçš„é‡åŒ–/è¯­è¨€æ™ºåŠ›æ°´å¹³ã€‚
è¿™ä¼¼ä¹ä¹ä¸€çœ‹å¾ˆç®€å•ï¼šåªè¦ä»åˆ†è§£çš„çŸ©é˜µä¸­å–å‡ºåˆ†æ•°å³å¯ã€‚ä¾‹å¦‚ï¼Œä¸Šé¢çš„åˆ†æè¡¨æ˜ï¼ŒAliceçš„é‡åŒ–æ™ºåŠ›æ°´å¹³æœ€é«˜ã€‚

However, this is incorrect, because the decomposition is not unique! The following is another valid decomposition.

ç„¶è€Œï¼Œè¿™æ˜¯ä¸æ­£ç¡®çš„ï¼Œå› ä¸ºåˆ†è§£å¹¶ä¸æ˜¯å”¯ä¸€ç¡®å®šçš„ï¼Œä¸‹é¢æ˜¯å¦ä¸€ç§åŒæ ·æˆç«‹çš„åˆ†è§£æ–¹å¼ï¼š

<div align=center>
<img src="img/2_1.png" />
</div>

According to this decomposition, Bob is strongest in quantitative intelligence, not Alice. Both decompositions explain the data perfectly and we cannot decide a priori which is correct.

åœ¨æ–°çš„åˆ†è§£æ–¹å¼ä¸­ï¼ŒBobçš„é‡åŒ–æ™ºåŠ›æ°´å¹³æœ€é«˜ï¼Œè€Œä¸æ˜¯Aliceã€‚ä¸¤ç§åˆ†è§£éƒ½å¾ˆå¥½çš„ä¸å®é™…æ•°æ®å»åˆï¼Œæ‰€ä»¥æˆ‘ä»¬æ ¹æœ¬æ— æ³•ç¡®å®šå“ªä¸€ä¸ªæ‰æ˜¯æ­£ç¡®çš„ã€‚

Sometimes we can hope to find the unique solution by imposing additional constraints on the decomposition, such as all matrix entries have to be nonnegative. However even after imposing many natural constraints, in general the issue of multiple decompositions will remain.

é€šå¸¸ï¼Œæˆ‘ä»¬å¸Œæœ›é€šè¿‡åœ¨åˆ†è§£è¿‡ç¨‹ä¸­é™„åŠ çº¦æŸæ‰¾åˆ°å”¯ä¸€çš„è§£ï¼Œä¾‹å¦‚æ‰€æœ‰çŸ©é˜µå…ƒç´ å¿…é¡»æ˜¯éè´Ÿçš„ã€‚ç„¶è€Œï¼Œå°½ç®¡åˆ©ç”¨äº†è®¸å¤šæ­£å¸¸çš„çº¦æŸï¼Œé€šå¸¸å¤šè§£çš„é—®é¢˜ä¾æ—§å­˜åœ¨ã€‚

## 3. å¢åŠ ç¬¬ä¸‰ä¸ªç»´åº¦ï¼ˆAdding the 3rd Dimensionï¼‰

Since our current data has multiple explanatory decompositions, we need more data to learn exactly which explanation is the truth. Assume the strength of the intelligence changes with time: we get better at quantitative tasks at night. Now we can let the (poor) students take the tests twice: once during the day and once at night. The results we get can be represented by two matrices Mday and Mnight. But we can also think of this as a three dimensional array of numbers -â€“ a tensor T in Râ™¯studentsÃ—â™¯testsÃ—2. Here the third axis stands for â€œdayâ€ or â€œnightâ€. We say the two matrices Mday and Mnight are slices of the tensor T.

åœ¨ç¬¬äºŒèŠ‚æˆ‘ä»¬è®²åˆ°äº†ï¼Œå› ä¸ºç›®å‰çš„æ•°æ®æœ‰å¤šç§åˆ†è§£æ–¹å¼ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æ›´å¤šçš„æ•°æ®å»æŒ–æ˜åˆ°åº•æ€æ ·çš„åˆ†è§£æ‰æ˜¯æ­£ç¡®çš„ã€‚
å‡è®¾æ™ºåŠ›æ°´å¹³ä¼šéšç€ä¸€å¤©å†…çš„æ—¶é—´å‘ç”Ÿæ”¹å˜ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨æ™šä¸Šçš„æ—¶å€™å¯ä»¥æ›´å¥½çš„å®Œæˆé‡åŒ–ç±»å‹çš„å·¥ä½œï¼ˆåªæ˜¯ä¸€ä¸ªå‡è®¾ï¼‰ã€‚
ç°åœ¨è®©å­¦ç”Ÿè¿›è¡Œä¸¤æ¬¡è€ƒè¯•ï¼Œä¸€æ¬¡æ˜¯åœ¨ç™½å¤©ï¼Œä¸€æ¬¡åœ¨æ™šä¸Šã€‚å¾—åˆ°çš„ç»“æœå°†ä¼šç”¨ä¸¤ä¸ªçŸ©é˜µè¡¨ç¤ºï¼Œ<img src="http://latex.codecogs.com/gif.latex?\textbf{M}_{day}">å’Œ<img src="http://latex.codecogs.com/gif.latex?\textbf{M}_{night}">ã€‚
ä½†æ˜¯æˆ‘ä»¬å¯ä»¥è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªä¸‰ç»´çŸ©é˜µâ€”â€”å¼ é‡<img src="http://latex.codecogs.com/gif.latex?\textbf{T}%20\in%20\textbf{R}^{\sharp%20students%20\times%20\sharp%20tests%20\times2">ã€‚
è¿™é‡Œç¬¬ä¸‰ä¸ªç»´åº¦è¡¨ç¤ºç™½å¤©å’Œé»‘å¤œã€‚æ¢å¥è¯è¯´ï¼ŒçŸ©é˜µ<img src="http://latex.codecogs.com/gif.latex?\textbf{M}_{day}">å’Œ<img src="http://latex.codecogs.com/gif.latex?\textbf{M}_{night}">æ˜¯å¼ é‡<img src="http://latex.codecogs.com/gif.latex?\textbf{T}">åœ¨ç¬¬ä¸‰ä¸ªç»´åº¦æ–¹å‘ä¸Šçš„åˆ‡ç‰‡ã€‚

<div align=center>
<img src="img/3_1.png" />
</div>

Let zquant and zverb be the relative strength of the two kinds of intelligence at a particular time (day or night), then the new score can be computed by a trilinear function:

ç”¨<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{z}_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{z}_{verb}" /> è¡¨ç¤ºåœ¨ç‰¹å®šæ—¶é—´ï¼ˆç™½å¤©æˆ–è€…æ™šä¸Šï¼‰è¡¡é‡ä¸¤ç§æ™ºåŠ›æ°´å¹³ï¼Œæ–°çš„åˆ†æ•°å¯ä»¥ç”¨ä¸‰ç»´çº¿æ€§å‡½æ•°è¡¨ç¤ºï¼š

<div align=center>
<img src="http://latex.codecogs.com/gif.latex?score=x_{quant}%20\times%20y_{quant}%20\times%20z_{quant}+x_{verb}%20\times%20y_{verb}%20\times%20z_{verb}" />
</div>

Keep in mind that this is the formula for one entry in the tensor: the score of one student, in one test and at a specific time. Who the student is specifies xquant and xverb; what the test is specifies weights yquant and yverb; when the test takes place specifies zquant and zverb.

è®°ä½ï¼šè¿™ä¸ªå…¬å¼æ˜¯ç”¨æ¥è®¡ç®—å¼ é‡ä¸­çš„æŸä¸€ä¸ªå…ƒç´ çš„ï¼Œå³åœ¨æŸä¸€ä¸ªæ—¶é—´ä¸‹çš„ï¼ŒæŸä¸€ä¸ªå­¦ç”Ÿçš„æŸä¸€é¡¹è€ƒè¯•æˆç»©ã€‚
æ¯ä¸ªå­¦ç”Ÿéƒ½æœ‰å¯¹åº”çš„æ™ºåŠ›æ°´å¹³<img src="http://latex.codecogs.com/gif.latex?x_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?x_{verb}" /> ï¼Œæ¯ä¸ªè€ƒè¯•ç§‘ç›®éƒ½æœ‰å¯¹åº”çš„æ™ºåŠ›ç±»å‹æƒé‡<img src="http://latex.codecogs.com/gif.latex?y_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?y_{verb}" />ï¼Œè€ƒè¯•ç§‘ç›®çš„æ—¶é—´ä¹Ÿæœ‰å¯¹åº”çš„æ™ºåŠ›ç±»å‹æƒé‡<img src="http://latex.codecogs.com/gif.latex?z_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?z_{verb}" />ã€‚

Similar to matrices, we can view this as a rank 2 decomposition of the tensor T. In particular, if we use xâƒ— quant,xâƒ— verb to denote the strengths of students, yâƒ— quant,yâƒ— verb to denote the weights of the tests and zâƒ— quant,zâƒ— verb to denote the variations of strengths in time, then we can write the decomposition as

ç±»ä¼¼äºå‰é¢çš„çŸ©é˜µï¼Œæˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸ªçœ‹åšæ˜¯ç§©ä¸º2çš„å¼ é‡<img src="http://latex.codecogs.com/gif.latex?\textbf{T}">çš„åˆ†è§£ã€‚
ç‰¹åˆ«çš„ï¼Œå¦‚æœæˆ‘ä»¬ç”¨<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{x}_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{x}_{verb}" /> å‘é‡è¡¨ç¤ºå­¦ç”Ÿæ ·æœ¬çš„ä¸¤ç§æ™ºåŠ›æ°´å¹³ï¼Œ
<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{y}_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{y}_{verb}" />è¡¨ç¤ºåœ¨ä¸åŒè€ƒè¯•ä¸­ä¸¤ä¸ªæ™ºåŠ›ç±»å‹çš„æƒé‡ï¼Œ
<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{z}_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{z}_{verb}" />è¡¨ç¤ºåœ¨ä¸åŒçš„æ—¶é—´ä¸‹ä¸¤ä¸ªæ™ºåŠ›ç±»å‹çš„æƒé‡ï¼Œé‚£ä¹ˆå°±å¯ä»¥æŠŠåˆ†è§£è®°ä½œï¼š

<div align=center>
<img src="http://latex.codecogs.com/gif.latex?\textbf{T}=\overrightarrow{x}_{quant}%20\otimes%20\overrightarrow{y}_{quant}%20\otimes%20\overrightarrow{z}_{quant}+\overrightarrow{x}_{verb}%20\otimes%20\overrightarrow{y}_{verb}%20\otimes%20\overrightarrow{z}_{verb}" />
</div>

<div align=center>
<img src="img/3_2.png" />
</div>

Now we can check that the second matrix decomposition we had is no longer valid: there are no values of zquant and zverb at night that could generate the matrix Mnight. This is not a coincidence. Kruskal 1977 gave sufficient conditions for such decompositions to be unique. When applied to our case it is very simple:

ç°åœ¨æˆ‘ä»¬å¯ä»¥æ£€éªŒåœ¨ç¬¬2èŠ‚ä¸­å°†åˆ°çš„åˆ†è§£æ–¹å¼ä¸å†æˆç«‹ï¼Œå› ä¸ºæ— æ³•è§£å‡ºæ™šä¸Šçš„<img src="http://latex.codecogs.com/gif.latex?z_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?z_{verb}" />ï¼Œä»è€Œæ— æ³•ç”ŸæˆçŸ©é˜µ<img src="http://latex.codecogs.com/gif.latex?\textbf{M}_{night}">ã€‚è¿™ç»éå¶ç„¶ã€‚[Kruskal 1977](http://www.sciencedirect.com/science/article/pii/0024379577900696)ç»™å‡ºäº†çŸ©é˜µå¯ä»¥è¢«å”¯ä¸€åˆ†è§£çš„å……åˆ†æ¡ä»¶ï¼Œè€Œåº”ç”¨åˆ°æœ¬æ–‡ä¸­çš„æ¡ˆä¾‹éå¸¸ç®€å•ï¼š

Corollary The decomposition of tensor T is unique (up to scaling and permutation) if none of the vector pairs (xâƒ— quant,xâƒ— verb), (yâƒ— quant,yâƒ— verb), (zâƒ— quant,zâƒ— verb) are co-linear.

>  æ¨è®ºï¼šå¦‚æœå‘é‡å¯¹<img src="http://latex.codecogs.com/gif.latex?(\overrightarrow{x}_{quant},\overrightarrow{x}_{verb})"/>ï¼Œ<img src="http://latex.codecogs.com/gif.latex?(\overrightarrow{y}_{quant},\overrightarrow{y}_{verb})"/>ä»¥åŠ<img src="http://latex.codecogs.com/gif.latex?(\overrightarrow{z}_{quant},\overrightarrow{z}_{verb})"/>éƒ½æ˜¯éçº¿æ€§ç›¸å…³çš„ï¼Œé‚£ä¹ˆå¼ é‡<img src="http://latex.codecogs.com/gif.latex?\textbf{T}">çš„åˆ†è§£å°±æ˜¯å”¯ä¸€çš„ã€‚

Note that of course the decomposition is not truly unique for two reasons. First, the two tensor factors are symmetric, and we need to decide which factor correspond to quantitative intelligence. Second, we can scale the three components xâƒ— quant ,yâƒ— quant, zâƒ— quant simultaneously, as long as the product of the three scales is 1. Intuitively this is like using different units to measure the three components. Kruskalâ€™s result showed that these are the only degrees of freedom in the decomposition, and there cannot be a truly distinct decomposition as in the matrix case.

åˆ†è§£ä¸å”¯ä¸€çš„åŸå› ä¸»è¦æœ‰ä¸¤ç‚¹ï¼šå…¶ä¸€ï¼Œå¼ é‡çš„ä¸¤ä¸ªæˆåˆ†æ˜¯å¯¹ç§°çš„ï¼Œå¹¶ä¸”æˆ‘ä»¬éœ€è¦å†³å®šå“ªä¸ªæˆåˆ†å¯¹åº”äºé‡åŒ–æ™ºåŠ›æ°´å¹³ï¼›
å…¶äºŒï¼Œæˆ‘ä»¬å¯ä»¥åŒæ—¶ç¼©æ”¾ä¸‰ä¸ªç»´åº¦<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{x}_{quant}" />ï¼Œ<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{y}_{quant}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{z}_{quant}" />ï¼Œåªè¦æ»¡è¶³ä¸‰ä¸ªå‘é‡ä¹˜ç§¯çš„ç¼©æ”¾æ¯”ä¾‹ä¿æŒ1å³å¯ã€‚
ç›´è§‚ä¸Šï¼Œè¿™å°±åƒæ˜¯ç”¨ä¸åŒçš„è®¡é‡å•ä½å»æµ‹é‡ä¸‰ä¸ªå‘é‡ã€‚
Kruskalçš„ç»“æœè¡¨æ˜ï¼Œåªæœ‰åˆ†è§£çš„é˜¶æ•°æ˜¯è‡ªç”±é‡ï¼Œå¹¶ä¸”åœ¨çŸ©é˜µåˆ†è§£ä¸­ä¸ä¼šæœ‰æœ¬è´¨ä¸Šçš„åŒºåˆ«ã€‚

## 4. æŒ–æ˜å¼ é‡ï¼ˆFinding the Tensorï¼‰

In the above example we get a low rank tensor T by gathering more data. In many traditional applications the extra data may be unavailable or hard to get. Luckily, many exciting recent developments show that we can uncover these special tensor structures even if the original data is not in a tensor form!

åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­æˆ‘ä»¬é€šè¿‡è·å¾—æ›´å¤šçš„æ•°æ®æ¥æ„é€ ä¸€ä¸ªä½ç§©å¼ é‡<img src="http://latex.codecogs.com/gif.latex?\textbf{T}">ã€‚
åœ¨å¾ˆå¤šæƒ…å†µä¸‹ï¼Œé¢å¤–çš„æ•°æ®å¾€å¾€æ˜¯ä¸èƒ½æˆ–è€…å¾ˆéš¾è·å–çš„ã€‚å¹¸è¿çš„æ˜¯ï¼Œç›®å‰å¾ˆå¤šä»¤äººæ¿€åŠ¨çš„æ–¹æ³•è¡¨æ˜å°½ç®¡åŸå§‹æ•°æ®å¹¶ä¸æ˜¯ä¸€ä¸ªå¼ é‡çš„å½¢å¼ï¼Œæˆ‘ä»¬å¯ä»¥æŒ–æ˜è¿™äº›ç‰¹æ®Šçš„å¼ é‡ç»“æ„ã€‚

The main idea is to use method of moments (see a nice post by Moritz): estimate lower order correlations of the variables, and hope these lower order correlations have a simple tensor form.

å…¶ä¸»è¦æ€æƒ³æ˜¯ä½¿ç”¨çŸ©é‡æ³•ï¼ˆå¯ä»¥å‚è€ƒMoritzçš„[åšå®¢](http://blog.mrtz.org/2014/04/22/pearsons-polynomial.html)ï¼‰ï¼Œè®¡ç®—å˜é‡çš„ä½é˜¶ç›¸å…³æ€§ï¼Œå¹¶ä¸”å¸Œæœ›å…¶ç›¸å…³æ€§å¯ä»¥æœ‰ä¸€ä¸ªç®€å•çš„å¼ é‡å½¢å¼ã€‚

Consider Hidden Markov Model as an example. Hidden Markov Models are widely used in analyzing sequential data like speech or text. Here for concreteness we consider a (simplified) model of natural language texts(which is a basic version of the word embeddings).

è¿™é‡Œä»¥éšè—é©¬å°”ç§‘å¤«ï¼ˆHMMï¼‰æ¨¡å‹ä¸ºä¾‹ã€‚HMMå¹¿æ³›ç”¨äºåˆ†æåºåˆ—æ•°æ®ï¼Œæ¯”å¦‚è¯­éŸ³æˆ–è€…æ–‡æœ¬ã€‚
è¿™é‡Œæˆ‘ä»¬å…·ä½“è€ƒè™‘ä¸€ä¸ªç®€åŒ–çš„è‡ªç„¶è¯­è¨€æ¨¡å‹ï¼ˆä¹Ÿæ˜¯æœ€åŸºæœ¬çš„è¯å‘é‡æ¨¡å‹ï¼‰ã€‚

In Hidden Markov Model, we observe a sequence of words (a sentence) that is generated by a walk of a hidden Markov Chain: each word has a hidden topic h (a discrete random variable that specifies whether the current word is talking about â€œsportsâ€ or â€œpoliticsâ€); the topic for the next word only depends on the topic of the current word. Each topic specifies a distribution over words. Instead of the topic itself, we observe a random word x drawn from this topic distribution (for example, if the topic is â€œsportsâ€, we will more likely see words like â€œscoreâ€). The dependencies are usually illustrated by the following diagram:

åœ¨HMMä¸­ï¼Œæˆ‘ä»¬è§‚æµ‹ä¸€ä¸ªç”±éšè—é©¬å°”ç§‘å¤«é“¾æ‰€ç”Ÿæˆçš„è¯æ±‡åºåˆ—ï¼ˆä¹Ÿå°±æ˜¯ä¸€ä¸ªå¥å­ï¼‰ï¼šæ¯ä¸ªå•è¯éƒ½æœ‰ä¸€ä¸ªéšè—çš„è¯é¢˜ <img src="http://latex.codecogs.com/gif.latex?h">ï¼ˆä¸€ä¸ªåˆ†ç¦»çš„éšæœºçš„å˜é‡ï¼ŒæŒ‡å®šäº†å½“å‰è¯æ±‡è®¨è®ºçš„æ˜¯â€œè¿åŠ¨â€æˆ–è€…â€œæ”¿æ²»â€ï¼‰ï¼Œ
ä¸‹ä¸€ä¸ªè¯çš„è¯é¢˜åªå–å†³äºå½“å‰è¯çš„è¯é¢˜ï¼Œæ¯ä¸ªè¯é¢˜éƒ½å¯¹åº”äº†ä¸€äº›è¯æ±‡çš„åˆ†å¸ƒã€‚
é™¤äº†è¯é¢˜æœ¬èº«ï¼Œæˆ‘ä»¬è§‚æµ‹åˆ°çš„éšæœºçš„è¯æ±‡ <img src="http://latex.codecogs.com/gif.latex?x">æ˜¯ä»è¯é¢˜çš„åˆ†å¸ƒä¸­æå–å‡ºçš„
ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœè¯é¢˜æ˜¯å…³äºè¿åŠ¨ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¾ˆå¯èƒ½ä¼šçœ‹åˆ°ç±»ä¼¼äºâ€å¾—åˆ†â€œè¿™æ ·çš„å•è¯ï¼‰ã€‚è¿™ç§ä¾èµ–æ€§é€šå¸¸ç”¨ä¸‹å›¾é˜é‡Šï¼š

<div align=center>
<img src="img/4_1.png" />
</div>

More concretely, to generate a sentence in Hidden Markov Model, we start with some initial topic h1. This topic will evolve as a Markov Chain to generate the topics for future words h2,h3,â€¦,ht. We observe words x1,â€¦,xt from these topics. In particular, word x1 is drawn according to topic h1, word x2 is drawn according to topic h2 and so on.

æ›´å…·ä½“çš„ï¼Œä¸ºäº†ä»¥HMMçš„å½¢å¼ç”Ÿæˆä¸€ä¸ªå¥å­ï¼Œæˆ‘ä»¬ä¼šä»¥ä¸€ä¸ªåˆå§‹è¯é¢˜<img src="http://latex.codecogs.com/gif.latex?h_1">å¼€å§‹ã€‚
è¿™ä¸ªè¯é¢˜å°†ä¼šä»¥é©¬å°”ç§‘å¤«é“¾çš„å½¢å¼æ¼”å˜ï¼Œä»è€Œä¸ºæœªæ¥çš„å•è¯ç”Ÿæˆä¸€ç³»åˆ—è¯é¢˜<img src="http://latex.codecogs.com/gif.latex?h_2,h_3,...,h_t">ã€‚
æˆ‘ä»¬å°†ä¼šä»è¿™äº›è¯é¢˜å½“ä¸­è§‚æµ‹åˆ°ä¸€ç³»åˆ—è¯æ±‡<img src="http://latex.codecogs.com/gif.latex?x_1,x_2,...,x_t">ã€‚å…·ä½“çš„ï¼Œå•è¯<img src="http://latex.codecogs.com/gif.latex?x_1">æ˜¯ç”±è¯é¢˜<img src="http://latex.codecogs.com/gif.latex?h_1">ç”Ÿæˆçš„ï¼Œ<img src="http://latex.codecogs.com/gif.latex?x_2">æ˜¯ç”±è¯é¢˜<img src="http://latex.codecogs.com/gif.latex?h_2">ç”Ÿæˆçš„ï¼Œä»¥æ­¤ç±»æ¨ã€‚

Given many sentences that are generated exactly according to this model, how can we construct a tensor? A natural idea is to compute correlations: for every triple of words (i,j,k), we count the number of times that these are the first three words of a sentence. Enumerating over i,j,k gives us a three dimensional array (a tensor) T. We can further normalize it by the total number of sentences. After normalization the (i,j,k)-th entry of the tensor will be an estimation of the probability that the first three words are (i,j,k). For simplicity assume we have enough samples and the estimation is accurate:

é‚£ä¹ˆç»™å‡ºä¸€äº›ç‰¹åˆ«æ˜¯ç”±è¿™æ ·çš„æ¨¡å‹ç”Ÿæˆçš„å¥å­ï¼Œæˆ‘ä»¬æ€æ ·æ‰èƒ½æ„é€ ä¸€ä¸ªå¼ é‡å‘¢ï¼Ÿ
ä¸€ä¸ªå¾ˆè‡ªç„¶çš„æƒ³æ³•å°±æ˜¯è®¡ç®—ç›¸å…³æ€§ï¼šå¯¹äºæ¯ä¸€ä¸ªå•è¯ä¸‰å…ƒç»„<img src="http://latex.codecogs.com/gif.latex?(i,j,k)">ï¼Œ
æˆ‘ä»¬éƒ½ä¼šç´¯è®¡å¥å­çš„å‰ä¸‰ä¸ªå•è¯çš„æ¬¡æ•°ã€‚
æšä¸¾<img src="http://latex.codecogs.com/gif.latex?i,j,k">ï¼Œå°†ä¼šç”Ÿæˆä¸€ä¸ªä¸‰ç»´çš„å¼ é‡<img src="http://latex.codecogs.com/gif.latex?\textbf{T}">ã€‚
æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ç”¨å¥å­çš„æ€»æ•°æ ‡å‡†åŒ–è¯¥å¼ é‡ã€‚
ç»è¿‡æ ‡å‡†åŒ–åå¼ é‡ä¸­ç¬¬<img src="http://latex.codecogs.com/gif.latex?(i,j,k)">ä¸ªå…ƒç´ å°±æ˜¯â€œå‡ºç°å‰ä¸‰ä¸ªè¯ä¸º<img src="http://latex.codecogs.com/gif.latex?(i,j,k)">æƒ…å†µâ€çš„æ¦‚ç‡ã€‚
ä¸ºäº†é—®é¢˜ç®€åŒ–ï¼Œå‡è®¾æˆ‘ä»¬æœ‰è¶³å¤Ÿå¤šçš„æ ·æœ¬å¹¶ä¸”ä¼°è®¡æ˜¯å‡†ç¡®çš„ï¼š

<div align=center>
<img src="http://latex.codecogs.com/gif.latex?\textbf{T}=Pr[x_1=i,x_2=j,x_3=k]">
</div>

Why does this tensor have the nice low rank property? The key observation is that if we â€œfixâ€ (condition on) the topic of the second word h2, it cuts the graph into three parts: one part containing h1,x1, one part containing x2 and one part containing h3,x3. These three parts are independent conditioned on h2. In particular, the first three words x1,x2,x3 are independent conditioned on the topic of the second word h2. Using this observation we can compute each entry of the tensor as

é‚£ä¹ˆä¸ºä»€ä¹ˆè¿™ä¸ªå¼ é‡å…·æœ‰å¾ˆå¥½çš„ä½ç§©æ€§å‘¢ï¼Ÿ
å…³é”®çš„è§‚å¯Ÿåœ¨äºå¦‚æœæˆ‘ä»¬å›ºå®šç¬¬äºŒä¸ªå•è¯çš„è¯é¢˜<img src="http://latex.codecogs.com/gif.latex?h_2">ï¼Œé‚£ä¹ˆå°±ä¼šå°†æ‹“æ‰‘åˆ†è§£ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š
ä¸€ä¸ªéƒ¨åˆ†åŒ…æ‹¬äº†<img src="http://latex.codecogs.com/gif.latex?h_1,x_1">ï¼Œä¸€ä¸ªåŒ…æ‹¬äº†<img src="http://latex.codecogs.com/gif.latex?x_2">ä»¥åŠç¬¬ä¸‰ä¸ªåŒ…æ‹¬äº†<img src="http://latex.codecogs.com/gif.latex?h_3,x_3">ã€‚
è¿™ä¸‰ä¸ªéƒ¨åˆ†åœ¨å›ºå®š<img src="http://latex.codecogs.com/gif.latex?h_2">çš„æƒ…å†µä¸‹éƒ½æ˜¯äº’ç›¸ç‹¬ç«‹çš„ã€‚
ç‰¹åˆ«çš„ï¼Œä¸‰ä¸ªè¯<img src="http://latex.codecogs.com/gif.latex?x_1,x_2,x_3">åœ¨å›ºå®š<img src="http://latex.codecogs.com/gif.latex?h_2">çš„æƒ…å†µä¸‹æ˜¯ç‹¬ç«‹çš„ã€‚æ ¹æ®è¿™æ ·çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å¼ é‡ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼š

<div align=center>
<img src="http://latex.codecogs.com/gif.latex?\textbf{T}_{i,j,k}=\sum_{l=1}^{n}{Pr[h2=l]\times%20Pr[x_1=i|h_2=l]\times%20Pr[x_2=j|h_2=l]\times%20Pr[x_3=k|h_2=l]}">
</div>

Now if we let x l be a vector whose i-th entry is the probability of the first word is i, given the topic of the second word is l; let yâƒ— l and zâƒ— l be similar for the second and third word. We can then write the entire tensor as

ç°åœ¨ï¼Œå¦‚æœä»¤<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{x}_{l}" />æ˜¯ä¸€ä¸ªå‘é‡ï¼Œå…¶ç¬¬<img src="http://latex.codecogs.com/gif.latex?i" />ä¸ªå…ƒç´ ä¸ºï¼šåœ¨ç»™å®šç¬¬äºŒä¸ªå•è¯çš„è¯é¢˜æ˜¯<img src="http://latex.codecogs.com/gif.latex?l" />çš„æƒ…å†µä¸‹ç¬¬ä¸€ä¸ªè¯æ˜¯<img src="http://latex.codecogs.com/gif.latex?i" />çš„æ¦‚ç‡ã€‚
åŒæ ·çš„<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{y}_{l}" />å’Œ<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{z}_{l}" />åˆ†åˆ«å¯¹åº”ç¬¬äºŒä¸ªå’Œç¬¬ä¸‰ä¸ªè¯ã€‚
æˆ‘ä»¬å¯ä»¥æŠŠå¼ é‡<img src="http://latex.codecogs.com/gif.latex?\textbf{T}">å†™ä½œï¼š

<div align=center>
<img src="http://latex.codecogs.com/gif.latex?\textbf{T}=\sum_{r=1}^{n}{Pr[h_2=l]\overrightarrow{x}_{l}\otimes%20\overrightarrow{y}_{l}\otimes%20\overrightarrow{z}_{l}">
</div>

This is exactly the low rank form we are looking for! Tensor decomposition allows us to uniquely identify these components, and further infer the other probabilities we are interested in. For more details see the paper by Anandkumar et al. 2012 (this paper uses the tensor notations, but the original idea appeared in the paper by Mossel and Roch 2006).

è¿™å°±æ˜¯æˆ‘ä»¬è‹¦è‹¦å¯»æ‰¾çš„ä½ç§©å½¢å¼ï¼
å¼ é‡åˆ†è§£å¾—ä»¥è®©æˆ‘ä»¬å¯ä»¥è¾¨åˆ«è¿™äº›æˆåˆ†ï¼Œå¹¶ä¸”è¿›ä¸€æ­¥æ¨ç†å…¶ä»–æˆ‘ä»¬æ„Ÿå…´è¶£çš„æ¦‚ç‡ã€‚
æ›´å¤šçš„ç»†èŠ‚å¯ä»¥æŸ¥çœ‹[Anandkumar et al. 2012](http://arxiv.org/abs/1210.7559)
ï¼ˆè¿™ç¯‡è®ºæ–‡ç”¨åˆ°äº†å¼ é‡çš„ç¬¦å·ï¼Œä½†æ˜¯åŸåˆ›çš„æ€æƒ³æ—©åœ¨[Mossel and Roch 2006](https://projecteuclid.org/euclid.aoap/1151592244)å°±å‡ºç°äº†ï¼‰ã€‚

## 5. å¼ é‡åˆ†è§£çš„å®ç°ï¼ˆImplementing Tensor Decompositionï¼‰

Using method of moments, we can discover nice tensor structures from many problems. The uniqueness of tensor decomposition makes these tensors very useful in learning the parameters of the models. But how do we compute the tensor decompositions?

æ ¹æ®çŸ©é‡æ³•ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å¾ˆå¤šé—®é¢˜ä¸­å‘ç°å¾ˆæ£’ğŸ‘çš„å¼ é‡å½¢å¼ã€‚
å”¯ä¸€çš„å¼ é‡åˆ†è§£ä½¿å¾—å¾ˆå¤šå¼ é‡åœ¨å­¦ä¹ æ¨¡å‹å‚æ•°æ–¹é¢å¾ˆæœ‰ä»·å€¼ã€‚
ä½†æ˜¯æˆ‘ä»¬æ€ä¹ˆæ ·è®¡ç®—å¼ é‡åˆ†è§£å‘¢ï¼Ÿ

In the worst case we have bad news: most tensor problems are NP-hard. However, in most natural cases, as long as the tensor does not have too many components, and the components are not adversarially chosen, tensor decomposition can be computed in polynomial time! Here we describe the algorithm by Dr. Robert Jenrich (it first appeared in a 1970 working paper by Harshman, the version we present here is a more general version by Leurgans, Ross and Abel 1993).

ä¸å¹¸çš„æ˜¯ï¼Œåœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œå¤§å¤šæ•°å¼ é‡é—®é¢˜éƒ½æ˜¯NPéš¾é—®é¢˜ã€‚
ä½†æ˜¯ï¼Œåœ¨å¤šæ•°è‡ªç„¶çš„æƒ…å†µä¸‹ï¼Œåªè¦å¼ é‡æ²¡æœ‰å¤ªå¤šçš„æˆåˆ†ï¼Œå¹¶ä¸”æˆåˆ†ä¸æ˜¯æ•Œå¯¹åœ°é€‰æ‹©ï¼Œé‚£ä¹ˆå¼ é‡åˆ†è§£å¾€å¾€å¯ä»¥åœ¨å¤šé¡¹å¼ç±»å‹çš„æ—¶é—´å¤æ‚åº¦å†…è®¡ç®—å®Œæ¯•ã€‚
è¿™é‡Œæˆ‘ä»¬ä»‹ç»Dr. Robert Jenrichçš„ç®—æ³•ï¼ˆé¦–æ¬¡å‡ºç°åœ¨1970å¹´[Harshmançš„è®ºæ–‡](http://hbanaszak.mjr.uw.edu.pl/TempTxt/Harshman_1970_Foundations%20of%20PARAFAC%20Procedure%20MOdels%20and%20Conditions%20for%20an%20Expalanatory%20Multimodal%20Factor%20Analysis.pdf)å½“ä¸­ï¼Œè¿™é‡Œæˆ‘ä»¬ä»‹ç»çš„æ˜¯æ›´æ™®é€‚çš„[Leurgans, Ross and Abel 1993](http://dl.acm.org/citation.cfm?id=173234)ç‰ˆæœ¬ï¼‰ã€‚

Jenrichçš„ç®—æ³•ï¼š

è¾“å…¥ï¼šå¼ é‡<img src="http://latex.codecogs.com/gif.latex?\textbf{T}=\sum_{i=1}^{r}{\lambda_i%20\overrightarrow{x}_{i}\otimes%20\overrightarrow{y}_{i}\otimes%20\overrightarrow{z}_{i}">

1. æŒ‘é€‰ä¸¤ä¸ªéšæœºå‘é‡<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{u},\overrightarrow{v}">
2. è®¡ç®—<img src="http://latex.codecogs.com/gif.latex?\textbf{T}_{\overrightarrow{u}}=\sum_{i=1}^{n}{u_i%20\textbf{T[:,:,i]}=\sum_{i=1}^{r}{\lambda_i(\overrightarrow{u}^T%20\overrightarrow{z}_i)\overrightarrow{x}_i\overrightarrow{y}_i^T}">
3. è®¡ç®—<img src="http://latex.codecogs.com/gif.latex?\textbf{T}_{\overrightarrow{v}}=\sum_{i=1}^{n}{v_i%20\textbf{T[:,:,i]}=\sum_{i=1}^{r}{\lambda_i(\overrightarrow{v}^T%20\overrightarrow{z}_i)\overrightarrow{x}_i\overrightarrow{y}_i^T}">
4. <img src="http://latex.codecogs.com/gif.latex?\overrightarrow{u}_i,\overrightarrow{v}_i">åˆ†åˆ«æ˜¯<img src="http://latex.codecogs.com/gif.latex?\textbf{T}_{\overrightarrow{u}}(\textbf{T}_{\overrightarrow{v}})^{+}">å’Œ<img src="http://latex.codecogs.com/gif.latex?\textbf{T}_{\overrightarrow{v}}(\textbf{T}_{\overrightarrow{u}})^{+}">çš„ç‰¹å¾å‘é‡ã€‚

In the algorithm, â€œ+â€ denotes pseudo-inverse of a matrix (think of it as inverse if this is not familiar).

åœ¨ç®—æ³•ä¸­ï¼Œ<img src="http://latex.codecogs.com/gif.latex?%20^{+}">æŒ‡çš„æ˜¯çŸ©é˜µçš„å‡é€†ï¼ˆå¦‚æœä½ ä¸ç†Ÿæ‚‰çš„è¯ï¼Œå¯ä»¥è®¤ä¸ºå°±æ˜¯æ±‚é€†çŸ©é˜µçš„ç¬¦å·ï¼‰ã€‚

The algorithm looks at weighted slices of the tensor: a weighted slice is a matrix that is the projection of the tensor along the z direction (similarly if we take a slice of a matrix M, it will be a vector that is equal to Muâƒ— ). Because of the low rank structure, all the slices must share matrix decompositions with the same components.

ç®—æ³•å¯ä»¥çœ‹åšæ˜¯ç»™å¼ é‡çš„åˆ‡ç‰‡ä¸€å®šçš„æƒé‡ï¼šä¸€ä¸ªæƒé‡åŒ–çš„åˆ‡ç‰‡å®é™…ä¸Šå°±æ˜¯å¼ é‡æ²¿ç€<img src="http://latex.codecogs.com/gif.latex?z">æ–¹å‘çš„æŠ•å½±ï¼ˆç±»ä¼¼åœ°ï¼Œå–ä¸€ä¸ªçŸ©é˜µ<img src="http://latex.codecogs.com/gif.latex?\textbf{M}">çš„åˆ‡ç‰‡å®é™…ä¸Šå°±ç­‰æ•ˆäº<img src="http://latex.codecogs.com/gif.latex?\textbf{M}\overrightarrow{u}">è®¡ç®—å¾—åˆ°çš„å‘é‡ï¼‰ã€‚ç”±äºå…¶ä½ç§©ç»“æ„ï¼Œæ‰€æœ‰çš„åˆ‡ç‰‡éƒ½å¿…é¡»å…±äº«å…·æœ‰åŒæ ·æˆåˆ†çš„çŸ©é˜µåˆ†è§£ã€‚

The main observation of the algorithm is that although a single matrix can have infinitely many low rank decompositions, two matrices can only have a unique decomposition if we require them to have the same components. In fact, it is highly unlikely for two arbitrary matrices to share decompositions with the same components. In the tensor case, because of the low rank structure we have

è¯¥ç®—æ³•çš„ä¸»è¦æ€è·¯æ˜¯å°½ç®¡å•ä¸€çŸ©é˜µå¯ä»¥æœ‰æ— é™ç§ä½ç§©åˆ†è§£çš„æƒ…å†µï¼Œä½†æ˜¯å¦‚æœè¦æ±‚æ‹¥æœ‰åŒæ ·çš„æˆåˆ†ï¼Œé‚£ä¹ˆä¸¤ä¸ªçŸ©é˜µåªèƒ½æœ‰å”¯ä¸€çš„åˆ†è§£ã€‚
äº‹å®ä¸Šï¼Œä¸¤ä¸ªä»»æ„çŸ©é˜µçš„åˆ†è§£åŸºæœ¬ä¸å¯èƒ½å…±ç”¨åŒæ ·çš„æˆåˆ†ã€‚
åœ¨å¼ é‡çš„æƒ…å†µä¸‹ï¼Œç”±äºä½ç§©çš„ç»“æ„ï¼Œæˆ‘ä»¬æœ‰ï¼š

<div align=center>
<img src="http://latex.codecogs.com/gif.latex?\textbf{T}_{\overrightarrow{u}}=\textbf{X}\textbf{D}_{\overrightarrow{u}}\textbf{Y}^T;\textbf{T}_{\overrightarrow{v}}=\textbf{X}\textbf{D}_{\overrightarrow{v}}\textbf{Y}^T">
</div>

where Duâƒ— ,Dvâƒ—  are diagonal matrices. This is called a simultaneous diagonalization for Tuâƒ—  and Tvâƒ— . With this structure it is easy to show that xâƒ— iâ€™s are eigenvectors of Tuâƒ— (Tvâƒ— )+=XDuâƒ— Dâˆ’1vâƒ— X+. So we can actually compute tensor decompositions using spectral decompositions for matrices.

ä¸Šå¼ä¸­ï¼Œ<img src="http://latex.codecogs.com/gif.latex?\textbf{D}_{\overrightarrow{u}},\textbf{D}_{\overrightarrow{v}}">å‡ä¸ºå¯¹è§’çŸ©é˜µã€‚
è¿™ç§°ä½œå¯¹äº<img src="http://latex.codecogs.com/gif.latex?\textbf{T}_{\overrightarrow{u}}">å’Œ<img src="http://latex.codecogs.com/gif.latex?\textbf{T}_{\overrightarrow{v}}">çš„åŒæ­¥å¯¹è§’åŒ–ã€‚
åœ¨è¿™æ ·çš„ç»“æ„ä¸­ï¼Œå¾ˆå®¹æ˜“çœ‹å‡º<img src="http://latex.codecogs.com/gif.latex?\overrightarrow{x}_i">å°±æ˜¯<img src="http://latex.codecogs.com/gif.latex?\textbf{T}_{\overrightarrow{u}}(\textbf{T}_{\overrightarrow{v}})^{+}=\textbf{X}\textbf{D}_{\overrightarrow{u}}\textbf{D}_{\overrightarrow{v}}^{-1}%20\textbf{Y}^{+}">çš„ç‰¹å¾å‘é‡ã€‚
æ‰€ä»¥å®é™…ä¸Šæˆ‘ä»¬å¯ä»¥é€šè¿‡çŸ©é˜µçš„è°±åˆ†è§£å®ç°å¼ é‡åˆ†è§£ã€‚

Many of the earlier works (including Mossel and Roch 2006) that apply tensor decompositions to learning problems have actually independently rediscovered this algorithm, and the word â€œtensorâ€ never appeared in the papers. In fact, tensor decomposition techniques are traditionally called â€œspectral learningâ€ since they are seen as derived from SVD. But now we have other methods to do tensor decompositions that have better theoretical guarantees and practical performances. See the survey by Kolda and Bader 2009 for more discussions.

å¾ˆå¤šæ—©å…ˆçš„åº”ç”¨å¼ é‡åˆ†è§£åˆ°å­¦ä¹ é—®é¢˜ä¸­çš„å·¥ä½œï¼ˆåŒ…æ‹¬[Mossel and Roch 2006](https://projecteuclid.org/euclid.aoap/1151592244)ï¼‰å·²ç»å‘ç°äº†è¿™ä¸ªç®—æ³•ï¼Œè€Œåè¯"tensor"å´ä»æœªå‡ºç°åœ¨è¯¥ç¯‡è®ºæ–‡ä¸­ã€‚
äº‹å®ä¸Šï¼Œå¼ é‡åˆ†è§£æŠ€æœ¯ä¼ ç»Ÿä¸Šè¢«ç§°ä¸ºè°±å­¦ä¹ ï¼Œå› ä¸ºä»–ä»¬æ˜¯ä»SVDæ–¹æ³•ä¸­æå–å‡ºæ¥çš„ã€‚
ä½†æ˜¯ç°åœ¨æˆ‘ä»¬æœ‰äº†å…¶ä»–æ–¹æ³•å¯ä»¥å®ç°å¼ é‡åˆ†è§£ï¼Œå¹¶ä¸”è¿™äº›æ–¹æ³•æœ‰ç€æ›´å¥½çš„ç†è®ºä¾æ®å’Œå®è·µä¸­çš„è¡¨ç°ã€‚å…³äºè¿™æ–¹é¢ï¼Œå¯ä»¥ä»[Kolda and Bader 2009](http://dl.acm.org/citation.cfm?id=1655230)çš„æ¦‚è¿°ä¸­çœ‹åˆ°æ›´å¤šçš„è®¨è®ºã€‚








